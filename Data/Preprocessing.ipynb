{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59299568",
   "metadata": {},
   "source": [
    "Since the data should be preprocessed for the Neural Network, k-Nearest Neighbours and the Logistic Regression in the same way, this notebook runs the preprocessing. The data will be saved as .csv files that can than be used to apply machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cb606",
   "metadata": {},
   "source": [
    "## Adult data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a905ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Please note that the data has been cleaned (=no missing/NaN values) in advance.\n",
    "This is done by Cleaning.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# change directory\n",
    "os.chdir('Adult')\n",
    "\n",
    "# load data\n",
    "adult_train = pd.read_csv('Adult_train.csv')\n",
    "adult_val = pd.read_csv('Adult_val.csv')\n",
    "\n",
    "def preprocess_dataset(adult):\n",
    "    # make binary labels for income column\n",
    "    adult['income'] = adult['income'].str.replace('<=50K', '0')\n",
    "    adult['income'] = adult['income'].str.replace('>50K', '1')\n",
    "    adult['income'] = adult['income'].astype(int)\n",
    "\n",
    "    # make array with labels, remove labels from dataframe\n",
    "    labels = adult['income'].copy()\n",
    "    # labels = np.array(labels)\n",
    "    adult = adult.drop(['income'], axis=1)\n",
    "\n",
    "    # use Min-max scaling for continuous features\n",
    "    adult[['age','capital_gain','capital_loss','hr_per_week']] = MinMaxScaler().fit_transform(adult[['age','capital_gain','capital_loss','hr_per_week']])\n",
    "\n",
    "    # use One-hot encoding for categorial features\n",
    "    adult = pd.get_dummies(adult,columns = ['type_employer','education','marital','occupation','relationship','race','sex','country'])\n",
    "    \n",
    "    return adult, labels\n",
    "\n",
    "# apply preprocessing to training and validation set\n",
    "adult_train, labels_train = preprocess_dataset(adult_train)\n",
    "adult_val, labels_val = preprocess_dataset(adult_val)\n",
    "\n",
    "set(adult_train.columns).difference(adult_val.columns)\n",
    "\n",
    "# since only 1 entry for entire set, remove this column\n",
    "adult_train.drop('country_Holand-Netherlands', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac0e5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult training set saved to csv!\n",
      "\n",
      "Adult validation set saved to csv!\n"
     ]
    }
   ],
   "source": [
    "# now save the .csv files\n",
    "adult_train.to_csv('Adult_train_data.csv', index=False)\n",
    "labels_train.to_csv('Adult_train_labels.csv', index=False)\n",
    "print(\"Adult training set saved to csv!\\n\")\n",
    "\n",
    "adult_val.to_csv('Adult_val_data.csv', index=False)\n",
    "labels_val.to_csv('Adult_val_labels.csv', index=False)\n",
    "print(\"Adult validation set saved to csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575a821",
   "metadata": {},
   "source": [
    "## Student performance data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a3e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fails before processing: 100\n",
      "Number of fails after processing: 100\n",
      "\n",
      "Length of training set: 432, length of validation set: 217.\n",
      "Training set and labels have same length: True\n",
      "Validation set and labels have same length: True\n"
     ]
    }
   ],
   "source": [
    "# set path\n",
    "os.chdir('../Student')\n",
    "\n",
    "# load data\n",
    "student = pd.read_csv('student-por.csv', delimiter=';')\n",
    "\n",
    "# for testing if conversion goes right\n",
    "print(f\"Number of fails before processing: {len(student[student.G3 < 10])}\")\n",
    "\n",
    "# convert student grade to pass or fail\n",
    "student.loc[student['G3'] < 10, 'G3'] = 0\n",
    "student.loc[student['G3'] > 9, 'G3'] = 1\n",
    "student['G3'] = student['G3'].astype(int)\n",
    "\n",
    "# for testing\n",
    "print(f\"Number of fails after processing: {len(student[student.G3 == 0])}\\n\")\n",
    "\n",
    "# use Min-max scaling for continuous features\n",
    "student[['age','absences','G1','G2']] = MinMaxScaler().fit_transform(student[['age','absences','G1','G2']])\n",
    "\n",
    "# split training data and label\n",
    "student_data = student.loc[:,student.columns != 'G3']\n",
    "student_target = student['G3']\n",
    "\n",
    "# use One-hot encoding for categorial features\n",
    "columns = student_data.columns.values.tolist()\n",
    "continous_columns = ['age','absences','G1','G2']\n",
    "categorial_columns = [feature for feature in columns if feature not in continous_columns]\n",
    "student_data = pd.get_dummies(student_data, columns = categorial_columns)\n",
    "\n",
    "# split into training and validation set, same ratio as Adult set\n",
    "student_train = student_data[:int(len(student_data) * (2/3))]\n",
    "student_val = student_data[int(len(student_data) * (2/3)):]\n",
    "grade_train = student_target[:int(len(student_data) * (2/3))]\n",
    "grade_val = student_target[int(len(student_data) * (2/3)):]\n",
    "\n",
    "# check if sets are equal\n",
    "print(f\"Length of training set: {len(student_train)}, length of validation set: {len(student_val)}.\")\n",
    "print(f\"Training set and labels have same length: {len(student_train) == len(grade_train)}\")\n",
    "print(f\"Validation set and labels have same length: {len(student_val) == len(grade_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7258847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student performance training set saved to csv!\n",
      "\n",
      "Student performance validation set saved to csv!\n"
     ]
    }
   ],
   "source": [
    "# now save the .csv files\n",
    "student_train.to_csv('student_train_data.csv', index=False)\n",
    "grade_train.to_csv('student_train_grade.csv', index=False)\n",
    "print(\"Student performance training set saved to csv!\\n\")\n",
    "\n",
    "# and for the validation set\n",
    "student_val.to_csv('student_val_data.csv', index=False)\n",
    "grade_val.to_csv('student_val_grade.csv', index=False)\n",
    "print(\"Student performance validation set saved to csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342b021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
